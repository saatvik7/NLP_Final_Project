%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
% \usepackage{acl}
\usepackage{times}
\usepackage{float}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage[justification=centering]{caption}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Sarcasm Detection in Twitter Data}



\author{Anan Aramthanapon \\
  Harvey Mudd College \\
  \texttt{aaramthanpon@hmc.edu} \\\And
  Tatsuki Kuze \\
  Harvey Mudd College \\
  \texttt{tkuze@hmc.edu} \\\And
  Saatvik Sejpal \\
  Harvey Mudd College \\
  \texttt{ssejpal@hmc.edu} \\
  }

\date{}
\begin{document}
\maketitle

\begin{abstract}
 The detection of figurative language devices such as sarcasm and irony is an increasingly important problem. We tackle this problem using the Twitter Dataset from the
 FigLang2020 Shared Task. Most previous tasks in this field tackle Sarcasm Detection using Deep Learning models like BERT, and also use context to improve results. This
 paper does not only that but also implements n-gram models both with and without context and discusses the differences in performance. Our results show that using BERT with
 context gives us extremely promising results; however bigrams also do surprisingly well for a non-Deep Learning based model.
\end{abstract}
\section{Introduction}

Sarcasm refers to a use of words to convey meaning opposite of what is mentioned by the speaker in a literal level. As such, being able to recognize the use of sarcasm is important in correctly identifying the speaker’s actual meanings, especially in context of applications like sentiment analysis. The classification task of sarcasm detection is considered a binary classification task, wherein the classifiers are provided as input utterances and possibly other contextual information to classify whether the utterance is sarcastic or non-sarcastic. 

In particular, this paper will explore the question of how providing additional contextual conversational information to a sarcasm classification system affects the quality of the
classification. Nine models will be explored in this paper, three of which will consist of the BERT model transformer with a sequence
classification linear layer on top (BertForSequenceClassification) ~\cite{devlin-etal-2019-bert}, and six of which will be based using a Naive Bayes classifier on n-grams features.

Of the
three BERT based models, one model will be trained purely on the potentially sarcastic comment that is being classified, one will be trained on both the potentially sarcastic text
and the preceding comments before that text. The final BERT model will take an alternative approach of sentiment analysis on both the response and conversational context utterances
and using the resulting sentiments to detect sarcasm. 

Of the six traditional n-grams based models, we have three models for unigrams, bigrams, and trigrams respectively trained on just the n-grams from the potentially sarcastic comment 
with tf-idf \cite{Salton:86} weight adjustments and classified with a Naive Bayes classifier. 
We have three models trained on the same features with tf-idf and the same classifier, but also including n-grams from the preceding comments before the potentially sarcastic
comment.

The nine models will be evaluated upon accuracy and F1 score, and further discussion will be made as to why
each model may have performed better or worse compared
to others.

The \href{https://github.com/EducationalTestingService/sarcasm/releases}{dataset} we intend to use for this task is a set of Twitter Data from \href{https://sites.google.com/view/figlang2020/}{FigLang 2020's Shared Task}. This dataset has the context for the tweet (the preceding comment), the potentially sarcastic response, and a label denoting whether or not a Tweet is sarcastic.

\section{Literature Review}
\subsection{Related Work}

One initial approach for such a task was done through a semi-supervised classification algorithm that used pattern extraction, selection and matching and punctuation-based features
for a kNN-like strategy \cite{10.5555/1870568.1870582}. The paper analyzed Amazon reviews and twitter data that used a gold standard through Mechanical Turk, where each utterance
was fed into the classification algorithm for a classification task. Another approach was taken in a research conducted on irony and sentiment analysis focusing on a corpus of
ironic similes, acting as sarcasm markers \cite{Veale10detectingironic}. It uses these markers to classify whether a simile is ironic or not depending on the lexical similarity
between the terms and other factors like the frequency of its occurrence on the web. There were also other approaches that utilized word embeddings for the classification task,
using distributional semantics methods and SVM classifiers. ~\cite{Ghosh2015SarcasticON} In this method, the authors target a particular word to determine whether the sense
associated with the word matches the remaining words in the utterance. While these methods have been successful in producing results that are beyond baselines, some more recent
authors have researched alternative approaches. 

One motivation for an alternative approach may be the problem that sarcasm detection is difficult. A paper was produced that noted the difficulty in sarcasm detection without context, which further implies that computers probably would also require more contextual information for a good sarcasm detection algorithm. ~\cite{Wallace2014HumansRC} The paper found that people when annotating comments for sarcasm requests more contextual information than for non-sarcastic comments. As such, it would make sense that contextual information would be a valid tool for analysis in classifying sarcasm through algorithms. 

Context, which refers to any further information than the utterance in itself, ~\cite{10.1145/3124420} was found to be used more and more frequently in recent papers ~\cite{Ghosh-Muresan}. One approach
utilized the user’s historical tweets to predict sarcasm, where the user’s past expression regarding the entities in the tweet of interest is utilized in the classification process
of the sarcasm in a tweet. ~\cite{khattri-etal-2015-sentiment} A majority voting based sentiment analysis regarding a target phrase within the tweet is used to determine the user’s
sentiment towards the target, and sarcasm is detected by observing whether the historical sentiment differs from the tweet’s current sentiment. Another approach utilizes
conversational context within Tweets in addition to the user’s historical sentiments, audience information and other contextual information. ~\cite{bamman2015contextualized}
Interestingly, the additional information regarding the audience did not have as much of an effect on the accuracy of the detection in comparison to the increase in accuracy from
authorial information. 
A different paper uses a CNN to perform user profiling to provide the model with more context on the corpora, which was found to be helpful in this task\cite{hazarika}.

An experiment similar to what we are proposing to do using a BERT encoder has been done in the past and has been shown to be highly effective \cite{van-hee-etal-2018-semeval}.
Using sentiment analysis for sarcasm detection has also been investigated by previous researchers \cite{abu-farha}.

We will further investigate how traditional models such as Bag of Words or n-grams compare compare to using BERT.

\subsection{Methods}
The paper is driven by an existing shared task from the 2nd Workshop on Figurative Language Processing (FigLang 2020) at ACL 2020, and will use the Twitter Training Dataset
provided in the shared task \cite{Ghosh-Muresan}. The Twitter Training Dataset consists of a series of 5000 tweets that were machine-tagged to be sarcastic if they contained the hashtags ``\#sarcasm'' or ``\#sarcastic''.  Each tweet in the dataset is separated into “response” utterances and “context” utterances. The “response” utterances correspond
to the actual utterances to be classified: the potentially sarcastic comment, while the “context” utterances are previous twitter posts to which the “response” utterance responded to. 

For the data to be used for the model, we first tokenized the corpora using the BertTokenizer in the pytorch-pretrained-bert library. The BertTokenizer is used to tokenize the
texts in the data into index numbers in the BERT vocabulary to be used for both the BERT and n-gram models. For models with context, we first joined the context utterances to the
response utterances before tokenizing. With the preprocess procedure finished, the data was first split upon training and validation data of sizes 4000 and 1000 respectively.

BERT-based models in this paper are using BertForSequenceClassification ~\cite{devlin-etal-2019-bert}, after the data is converted into tokens using the tokenizer, the array of indexes were padded or truncated to match the maximum sequence length, which for
this experiment was determined to be 128. Attention masks are then created for each data to also be used for BERT. 
The data then is converted into torch tensors, the required data type for our model. The torch tensors were then further converted into iterators with torch DataLoaders. The
optimizer used for the machine learning training was BertAdam ~\cite{devlin-etal-2019-bert}, the optimizer frequently used in BERT models. For each model, the model was trained
with 4 epochs in balancing time taken with results. 

For n-gram based models, after the data is converted into tokens using the tokenizer, n-grams are constructed from the array of tokens and the counts of each n-gram is calculated
for each instance, then weight balanced using tf-idf. We then train on this data using a Gaussian Naive Bayes classifier from the sklearn library.

For each of the three BERT models, the first model is the model that only is trained upon the “response” utterances. The second model uses the same approach, but with the merged “response” and “context” utterances.

The final model utilizes a different approach from the two previously mentioned models. It first utilizes a dataset of IMDB movie reviews ~\cite{maas-EtAl:2011:ACL-HLT2011}
containing annotated data for sentiment analysis. The dataset is comprised from 25k training set data that contains annotations on whether a movie review is positive or negative.
Using this data, a sentiment analysis model is constructed using the BertForSequenceClassification model with the pipeline mentioned previously. This model is then to be run on the
twitter dataset, where the “response” utterance and “context” utterance are run sentiment analysis classification separately. The two sentiments are then later compared, wherein
non-matching sentiments may suggest sarcastic within the “response” utterance. This approach is inspired by past works that discussed the connection between sarcasm and sentiment
analysis. ~\cite{Veale10detectingironic}

For each of the six n-gram based models, we will have 2 models each for unigrams, bigrams, and trigrams. One with just the ``response'' utterances, and one with the ``context'' and
``response'' utterances merged. 

We hope to gain insights about two main aspects: 
\begin{itemize}
  \item How does using context affect the accuracy of our model?
  \item How does using n-gram based models compare with using deep learning BERT based models?
  \item What is the optimal number of adjacent words to use for n-gram based models?
  \item Is sentiment analysis alone effective for detecting sarcasm?
\end{itemize}

By having models that use context and similar models that do not use context, we are able to determine if having context impacts the accuracy of our models. These sets of models will provide us with enough information to allow us to analyze the impact of different lengths of n-grams, impact of context, impact of sentiment analysis, and the difference between traditional n-gram models and BERT models for the task of sarcasm detection.

Each of the nine models are then evaluated through testing on a validation dataset of 1000 datapoints. Each outcome of the classification algorithm will be compared with the gold labels, to compute F1 score and accuracy for each model. 

% What we're gonna plot, what our dataset is (break it down), metrics we're gonna use (not just the generic ones), Features we will use (and how this relates to previous work in
% the field)...
\section{Results and Discussion}
The performance of each of the predictor models were evaluated on a test set of 1000 data points, which were randomly separated from the training set in the beginning of the
experiment. The test set was relatively balanced with 495 sarcastic and 505 non-sarcastic data points. 

\begin{center}
  \tabcolsep=0.03cm
  \begin{table}[H]
  \begin{tabular}{||c c c c c||} 
   \hline
   Model & F1 & Precision & Recall & Acc. \\ [0.5ex] 
   \hline\hline
   Response Only & 0.79 & 0.77 & 0.82 & 0.79 \\ 
   \hline
   \textbf{Context + Response} & \textbf{ 0.94 } & \textbf{ 0.96 } & \textbf{ 0.92 } & \textbf{ 0.94 } \\ 
   \hline
   Sentiment Analysis & 0.59 & 0.65 & 0.55 & 0.63 \\ [0.5ex] 
   \hline
  \end{tabular}
  \caption{Metrics for BERT Models}  
   \label{table:bertMetrics}
  \end{table}
  \tabcolsep=0.22cm
  \begin{table}[H]
  \begin{tabular}{||c c c c c||} 
   \hline
   Model & F1 & Precision & Recall & Acc. \\ [0.5ex] 
   \hline\hline
   Unigrams & 0.54 & 0.62 & 0.47 & 0.60 \\ 
   \hline
   \textbf{Bigrams}  & \textbf{ 0.72 } &   \textbf{0.71} & \textbf{0.73} & \textbf{0.72} \\ 
   \hline
   Trigrams & 0.70 & 0.70 & 0.71 & 0.70 \\ [0.5ex] 
   \hline
  \end{tabular}
  \caption{Metrics for BoW and Collocational Features with Context}  
   \label{table:bowContext}
  \end{table}
  \tabcolsep=0.22cm
  \begin{table}[H]
  \begin{tabular}{||c c c c c||} 
   \hline
   Model & F1 & Precision & Recall & Acc. \\ [0.5ex] 
   \hline\hline
   Unigrams & 0.67 & 0.69 & 0.66 & 0.68 \\ 
   \hline
   \textbf{Bigrams}  & \textbf{ 0.70 } &   \textbf{0.72} & 0.68 & \textbf{0.71} \\ 
   \hline
   Trigrams & 0.69 & 0.65 & \textbf{ 0.74 } & 0.67 \\ [0.5ex] 
   \hline
  \end{tabular}
  \caption{Metrics for BoW and Collocational Features without Context}  
   \label{table:bowNoContext}
  \end{table}
  \end{center}

As shown on Table~\ref{table:bertMetrics}, of the three BERT models,
Context + Response model did the best with a high value for each of the 4 evaluation metrics, with response following it and the sentiment analysis doing relatively poorly compared to the
other two. Broadly speaking, it makes sense that the two BERT models that were trained specifically on the sarcasm task did well, since they can be thought to properly capture the
language structures of the model through the pre-trained BERT model, and was further trained using the BERT sequence transformer for the specific task. The context not only is more
information to be used for the classification, but also matches the results of \cite{Wallace2014HumansRC} wherein they showed how context is crucial information even for humans in
sarcasm detection.

As for the sentiment analysis, the relatively poor performance could potentially be from either that sentiment analysis isn’t a great predictor tool for sarcasm,
or that the IMDB dataset on which the sentiment analysis predictor was trained on did not match well with the twitter data that was used for the task. This would make sense as the
types of text on Twitter and in movies are different enough to the point where this could make the sentiment classification inaccurate.
\par Looking at the Bag of Words/Collocational Features (unigrams, bigrams, trigrams) models that we used, we see from Table~\ref{table:bowContext} and Table~\ref{table:bowNoContext}
that bigrams overall did the best of all the n-gram models, and that context did not have a significant impact on the models' performances with the exception of unigrams. We found that adding context did not significantly improve any of the models, only improving the bigram model slightly and worsening the trigram model slightly. It did, however, make the unigram model significantly worse, dropping the F1 score from 0.67 to 0.54. This makes
sense as when combining the context and the response, our n-gram based models did not distinguish between n-grams that originated from the context and the response. As the context
is usually not sarcastic, adding n-grams from this to a potentially sarcastic set of n-grams could confuse the model and lead to worse results. This is especially true for unigrams
which analyze single words, and would require certain words to be classified as sarcastic words or non sarcastic ones. This is not as significant for bigrams and trigrams, likely
because these models also account for word order more than unigrams and thus would be able to better understand the role of the context n-grams. 

\section{Future Work}

While sentiment analysis alone may not be enough to effectively classify texts as sarcastic or not, it may be effective when combined with other features. Currently,
we are tagging responses as sarcastic if the sentiment of the response does not match the sentiment of the context and not taking into account other factors. In the future, we can
try to incorporate the features of sentiment analysis with other models such as our main BERT-based model and even n-grams based ones to see if this would improve its performance.

As mentioned previously, we believe that a weakness of our traditional n-gram based models is that they are unable to distinguish between n-grams that originate from the
``context'' utterance and the ``response'' utterance.  To make this distinction even clearer, we could also tag n-grams originating from the ``context'' and the ``response'' differently so that our models are able to tell them apart. 

Another important point to mention is that while making our n-gram models we did not remove stop words or punctuation while tokenizing the corpus. Removing stop words could
potentially improve our n-gram results; however, as we did use tf-idf in our models it may not make a large difference. 

We could also try weighing hashtags more heavily in our n-gram models as hashtags are distinct from normal text and are often used to share some kind of sentiment. Perhaps
assigning a different weight to hashtag tokens could improve the model's performace.

Furthermore, we should also discuss the possibility that the gold labels for our dataset may not be entirely accurate. As was mentioned in the Shared Task, these tags were machine
labeled based on whether the tweets contained the hashtags ``\#sarcasm'' or ``\#sarcastic''. Tagging sarcasm this way is not always accurate, and has actually been shown to be a
poor way of tagging for sarcasm \cite{opreaiSarcasm}. Additionally, this paper does not take into account the difference between perceived and intended sarcasm, which is an important
distinction to make in sarcasm detection \cite{oprea}, as something can be perceived as sarcastic but not intended to be, or vice versa.

We believe that an impactful use of Sarcasm Detection lies in helping people who are learning English or any other language for the first time. This is because Sarcasm is
especially difficult to pick up on for non-native speakers of a language. Thus, if our model is packaged with a language translation software it could be very helpful. Furthermore,
another possible application of our model could be to flag and remove potentially very sarcastic and demeaning comments from social media platforms.
\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021}


\end{document}
