%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Sarcasm Detection in Twitter Data}



\author{Anan Aramthanapon \\
  Harvey Mudd College \\
  \texttt{aaramthanpon@hmc.edu} \\\And
  Saatvik Sejpal \\
  Harvey Mudd College \\
  \texttt{ssejpal@hmc.edu} \\}

\date{}

\begin{document}
\maketitle

\section{Introduction}

\paragraph{} We propose the following Research Question: Given a dataset of a few thousand English language Tweets, how accurately can we predict whether or not a particular Tweet employs sarcasm?
Furthermore, does the accuracy of our model improve if we train it on some context for the Tweet in question?

The \href{https://github.com/EducationalTestingService/sarcasm/releases}{dataset} we intend to use for this task is a set of Twitter Data from \href{https://sites.google.com/view/figlang2020/}{FigLang 2020's Shared Task}. This dataset has the context for the tweet (the preceding comment), the potentially sarcastic response, and a label denoting whether or not a Tweet is sarcastic.

To answer the Research Question, we propose to explore using models such as Bag of Words with tf-idf and n-grams with tf-idf, and then use classifiers such as NaiveBayes to train and test on the data. 
We will also use Deep Learning based models such as BERT \cite{devlin2019bert} as used in \cite{Ghosh-Muresan} for a very similar task but on a different dataset. Such models have proven to be very effective for sarcasm detection in the past \cite{Ghosh-Muresan}.
Additionally, we will also explore the effect of context on sarcasm detection by providing the preceding text and the sarcastic response, and comparing the results of providing the preceding text to results without.


\section{Literature Review}
\subsection{Related Work}

The experiment performed in Abu Farha et al., 2021 An experiment similar to what we are proposing to do using a BERT encoder has been done in the past \cite{abu-farha}, but it is different in that it also combines the features of sentiment analysis in the model, and is done in the Arabic language. This is slightly more complex than what we are proposing to do, but there is a lot of useful information about using BERT for a sarcasm task from this paper. We will also use n-grams for our experiment as n-grams are a common feature used in sentiment analysis tasks, and this paper has shown that sentiment analysis features are helpful for detecting sarcasm.

A summary of sarcasm and irony detection as well as a subtask of breaking down and specifying the type of sarcasm and irony is available for SemEval-2018 Task 3 \cite{van-hee-etal-2018-semeval}. Our task is similar to this but will only tackle the main task of detecting sarcasm without the sub classifications. This is because the results of the paper show that it is difficult to detect the specific type of sarcasm detected at this point. Instead, we will further investigate whether using models such as Bag of Words and tf-idf or n-grams will help improve the model's performance, as this paper only discussed the overall results of the top-performing teams for the task which mostly used deep learning techniques to perform the classifications. We will compare how using basic NLP techniques compare to using BERT.

There has been research on exploring the task of Sarcasm Detection with the help of context \cite{Ghosh-Muresan}, \cite{hazarika}. One paper discussed modeling conversation context in Twitter Data/Discussion Forums, and found that using conversation context helps with sarcasm detection \cite{Ghosh-Muresan}. The paper makes use of models such as LSTMs with attention to understand context and make predictions. A different paper uses a CNN along with user profiling to provide the model with more context on the corpora \cite{hazarika}. The method we are proposing does not do user profiling, but we will explore whether the added context of the preceding comment before the sarcastic comment will help models better detect sarcasm.

Sarcasm is challenging to label because intended sarcasm is not always the same as perceived sarcasm \cite{oprea}. Because of this, even hand-labeled datasets for sarcasm are likely to be noisy as readers may perceive a text in a way that does not agree with the author's intentions. Additionally, automatically tagging sarcastic tweets through certain keywords such as ``\#sarcasm'' is not effective because this has also been shown to be very noisy and this also fails to capture many types of sarcasm \cite{oprea}. Because of this, we have decided to use a dataset which contains sarcasm labels by the authors themselves.

\subsection{Methods}
We will use the aforementioned dataset for this task \cite{Ghosh-Muresan}, which contains 5000 datapoints, each containing the preceding comment and the potentially sarcastic response pulled from Twitter, as well as the ground truth label for whether the response is sarcastic or not.

We plan to answer our research question by performing a series of experiments. Our first baseline experiment will be to use a model based on Bag Of Words along with tf-idf \cite{Salton:86} using the Naive Bayes classifier, which is a baseline also used in similar experiments in the past \cite{hazarika}. We will also experiment with a model based on n-grams and tf-idf. We will compare how these baseline models compare to a state-of-the-art deep learning BERT model \cite{devlin2019bert} for the same task. For each model, we will conduct two experiments: one with the preceding comment and the response, and one with just the response to see the impact context has on sarcasm detection.

% BERT here
Before we can perform any experiments or analysis, we need to process our data by splitting our training data into training and validation data. For our experiments, we decided to hold out 20\% of our training set as a validation set. After this, we convert our data into Tensors so that we can run models like BERT on it.

For our deep learning model, we plan to use a pre-trained BERT model. For this process, we plan to use the pre-trained BERT models included with Pytorch's Transformer library. More specifically, we plan to use bert-base-cased and bert-base-uncased to compare how capitalization plays a role in detecting sarcasm. For each model, we will use the tokenizer shipped with the BERT model we are using. 


We hope to gain insights about two main aspects: 
\begin{itemize}
  \item How does using context affect the accuracy of our model?
  \item How does using each of the different models affect the accuracy of sarcasm detection?
\end{itemize}
By having one model that uses context, and another model that does not use context, we are able to determine if having context actually impacts the accuracy of our models.

To evaluate the models, we will use the standard metrics such as accuracy, precision, and F1 score. 

To help visualize our results, we will have one figure that displays the words that show up most frequently in tweets that are categorized as sarcastic by our model, excluding stop words.
We will have another figure that summarizes the differences in our models' performances with and without context, as well as using our baseline n-gram model against BERT.
We will also add a table to discuss certain testing examples for which our models had the worst performance, and try to analyze why our model didn't do well on those specific cases. We will also compare these results between models to see if there are certain features in some types of sarcasm that make them hard to detect in general or if there are certain areas in which a certain model is not effective at predicting.

% What we're gonna plot, what our dataset is (break it down), metrics we're gonna use (not just the generic ones), Features we will use (and how this relates to previous work in the field)...

\bibliographystyle{acl_natbib}
\bibliography{anthology,acl2021}


\end{document}
